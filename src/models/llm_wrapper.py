import asyncio
import logging
from typing import Dict, Any, List, Optional
import openai
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

class LLMWrapper:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger("llm_wrapper")
        self.openai_client = None
        self.local_models = {}
        
        self._initialize_clients()
    
    def _initialize_clients(self):
        """Initialize LLM clients"""
        # OpenAI client
        if self.config.get('openai_api_key'):
            self.openai_client = openai.AsyncOpenAI(
                api_key=self.config['openai_api_key']
            )
        
        # Local models
        if self.config.get('use_local_models', False):
            self._load_local_models()
    
    def _load_local_models(self):
        """Load local LLM models"""
        try:
            # Load a small local model for basic tasks
            model_name = self.config.get('local_model', 'distilgpt2')
            self.local_models['text_generation'] = pipeline(
                'text-generation',
                model=model_name,
                tokenizer=model_name,
                device=0 if torch.cuda.is_available() else -1
            )
            self.logger.info(f"Loaded local model: {model_name}")
        except Exception as e:
            self.logger.warning(f"Failed to load local model: {str(e)}")
    
    async def generate_text(self, prompt: str, model: str = None, **kwargs) -> str:
        """Generate text using specified model"""
        try:
            if model and 'gpt' in model and self.openai_client:
                return await self._generate_openai(prompt, model, **kwargs)
            elif self.local_models.get('text_generation'):
                return await self._generate_local(prompt, **kwargs)
            else:
                # Fallback to simple rule-based generation
                return await self._generate_fallback(prompt)
                
        except Exception as e:
            self.logger.error(f"Text generation failed: {str(e)}")
            return await self._generate_fallback(prompt)
    
    async def _generate_openai(self, prompt: str, model: str, **kwargs) -> str:
        """Generate text using OpenAI API"""
        try:
            response = await self.openai_client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=kwargs.get('max_tokens', 1000),
                temperature=kwargs.get('temperature', 0.7)
            )
            return response.choices[0].message.content
        except Exception as e:
            self.logger.error(f"OpenAI generation failed: {str(e)}")
            raise
    
    async def _generate_local(self, prompt: str, **kwargs) -> str:
        """Generate text using local model"""
        try:
            # Run in thread to avoid blocking
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: self.local_models['text_generation'](
                    prompt,
                    max_length=kwargs.get('max_length', 200),
                    num_return_sequences=1,
                    temperature=kwargs.get('temperature', 0.7),
                    do_sample=True
                )[0]['generated_text']
            )
            return result
        except Exception as e:
            self.logger.error(f"Local generation failed: {str(e)}")
            raise
    
    async def _generate_fallback(self, prompt: str) -> str:
        """Fallback text generation"""
        # Simple rule-based response for demonstration
        if 'blog' in prompt.lower():
            return "This is a sample blog post generated by the AI system. In a production environment, this would be properly generated content with engaging headlines, structured sections, and a compelling call-to-action."
        elif 'social media' in prompt.lower():
            return "Check out our latest update! ðŸš€ #AI #Innovation"
        else:
            return "This is AI-generated content. The system is currently operating in fallback mode."
    
    async def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """Analyze text sentiment"""
        # Simplified sentiment analysis
        positive_words = ['good', 'great', 'excellent', 'amazing', 'love', 'wonderful']
        negative_words = ['bad', 'terrible', 'awful', 'hate', 'disappointing']
        
        text_lower = text.lower()
        positive_score = sum(1 for word in positive_words if word in text_lower)
        negative_score = sum(1 for word in negative_words if word in text_lower)
        
        total = positive_score + negative_score
        if total == 0:
            return {'positive': 0.5, 'negative': 0.5, 'neutral': 1.0}
        
        return {
            'positive': positive_score / total,
            'negative': negative_score / total,
            'neutral': 1.0 - (abs(positive_score - negative_score) / total)
        }
    
    async def extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """Extract keywords from text"""
        # Simplified keyword extraction
        words = text.lower().split()
        # Remove common stop words
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        content_words = [word for word in words if word not in stop_words and len(word) > 3]
        
        # Count frequency
        from collections import Counter
        word_freq = Counter(content_words)
        
        return [word for word, _ in word_freq.most_common(top_k)]
    
    async def summarize_text(self, text: str, max_length: int = 150) -> str:
        """Summarize text"""
        # Simplified summarization
        sentences = text.split('.')
        if len(sentences) <= 3:
            return text
        
        # Take first, middle, and last sentences
        summary_sentences = [sentences[0], sentences[len(sentences)//2], sentences[-1]]
        summary = '. '.join([s.strip() for s in summary_sentences if s.strip()])
        
        if len(summary) > max_length:
            summary = summary[:max_length-3] + '...'
        
        return summary
